# PointNetLK ç‚¹äº‘é…å‡†å¯¹æ¯”ç ”ç©¶é¡¹ç›®

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.0%2B-orange.svg)](https://pytorch.org/)

**PointNetLK ç‚¹äº‘é…å‡†ç®—æ³•å¯¹æ¯”ç ”ç©¶é¡¹ç›®** - æ•´åˆäº†**åŸç‰ˆPointNetLK**å’Œ**æ”¹è¿›ç‰ˆPointNetLK_Revisited**ï¼Œæ”¯æŒ**C3VDåŒ»å­¦æ•°æ®é›†**å’Œ**ModelNet40æ•°æ®é›†**ï¼Œæä¾›ç»Ÿä¸€çš„è®­ç»ƒã€æµ‹è¯•å’Œå¯¹æ¯”åˆ†ææ¡†æ¶ã€‚ğŸ†• **æ–°å¢å¯æ›¿æ¢ç‰¹å¾æå–å™¨æ”¯æŒ**ï¼ŒåŒ…æ‹¬AttentionNetã€CFormerã€Mamba3Dç­‰å…ˆè¿›ç‰¹å¾æå–å™¨ã€‚

[Xueqian Li](https://lilac-lee.github.io/), [Jhony Kaesemodel Pontes](https://jhonykaesemodel.com/), 
[Simon Lucey](https://www.adelaide.edu.au/directory/simon.lucey)

**CVPR 2021 (Oral)** | [è®ºæ–‡é“¾æ¥](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_PointNetLK_Revisited_CVPR_2021_paper.pdf) | [arXiv](https://arxiv.org/pdf/2008.09527.pdf)

| ModelNet40 | 3DMatch | KITTI |
|:-:|:-:|:-:|
| <img src="imgs/modelnet_registration.gif" width="172" height="186"/>| <img src="imgs/3dmatch_registration.gif" width="190" height="186"/> | <img src="imgs/kitti_registration.gif" width="200" height="166"/> |

---

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#-é¡¹ç›®æ¦‚è¿°)
- [åŠŸèƒ½ç‰¹æ€§](#-åŠŸèƒ½ç‰¹æ€§)
- [é¡¹ç›®æ¶æ„](#-é¡¹ç›®æ¶æ„)
- [ç¯å¢ƒé…ç½®](#-ç¯å¢ƒé…ç½®)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [æ•°æ®é›†æ”¯æŒ](#-æ•°æ®é›†æ”¯æŒ)
- [ç‰¹å¾æå–å™¨](#-ç‰¹å¾æå–å™¨)
- [è®­ç»ƒæŒ‡å—](#-è®­ç»ƒæŒ‡å—)
- [æµ‹è¯•æŒ‡å—](#-æµ‹è¯•æŒ‡å—)
- [æ€§èƒ½å¯¹æ¯”](#-æ€§èƒ½å¯¹æ¯”)
- [APIä½¿ç”¨æŒ‡å—](#-apiä½¿ç”¨æŒ‡å—)
- [æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)
- [è´¡çŒ®æŒ‡å—](#-è´¡çŒ®æŒ‡å—)
- [è®¸å¯è¯](#-è®¸å¯è¯)

---

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ª**ç‚¹äº‘é…å‡†ç®—æ³•å¯¹æ¯”ç ”ç©¶å¹³å°**ï¼Œä¸»è¦è§£å†³ä»¥ä¸‹ç ”ç©¶é—®é¢˜ï¼š

### ğŸ”¬ ç ”ç©¶ç›®æ ‡
1. **ç®—æ³•å¯¹æ¯”**: æ·±å…¥æ¯”è¾ƒåŸç‰ˆPointNetLKå’Œæ”¹è¿›ç‰ˆPointNetLKçš„æ€§èƒ½å·®å¼‚
2. **åŒ»å­¦åº”ç”¨**: åœ¨C3VDåŒ»å­¦å†…çª¥é•œæ•°æ®é›†ä¸ŠéªŒè¯ç®—æ³•çš„å®é™…åº”ç”¨æ•ˆæœ
3. **æ ‡å‡†åŸºå‡†**: åœ¨ModelNet40æ•°æ®é›†ä¸Šå»ºç«‹æ ‡å‡†æ€§èƒ½åŸºå‡†
4. **æŠ€æœ¯åˆ›æ–°**: æ¢ç´¢ä½“ç´ åŒ–ã€é›…å¯æ¯”è®¡ç®—ç­‰å…³é”®æŠ€æœ¯çš„æœ€ä½³å®è·µ
5. ğŸ†• **ç‰¹å¾æå–å™¨ç ”ç©¶**: æ¯”è¾ƒä¸åŒç‰¹å¾æå–å™¨ï¼ˆPointNetã€Attentionã€CFormerã€Mamba3Dç­‰ï¼‰çš„é…å‡†æ€§èƒ½

### ğŸ† æ ¸å¿ƒè´¡çŒ®
- **ğŸ”„ ç»Ÿä¸€æ¡†æ¶**: æ•´åˆä¸¤ä¸ªç‰ˆæœ¬çš„PointNetLKï¼Œæä¾›ä¸€è‡´çš„APIæ¥å£
- **ğŸ¥ åŒ»å­¦åº”ç”¨**: é¦–æ¬¡åœ¨C3VDåŒ»å­¦æ•°æ®é›†ä¸Šè¯„ä¼°PointNetLKæ€§èƒ½
- **ğŸ“Š è¯¦ç»†å¯¹æ¯”**: æä¾›é›…å¯æ¯”è®¡ç®—æ–¹æ³•ï¼ˆæ•°å€¼ vs è§£æï¼‰çš„æ·±å…¥åˆ†æ
- **ğŸš€ æ€§èƒ½ä¼˜åŒ–**: å®ç°ä½“ç´ åŒ–ã€æ™ºèƒ½é‡‡æ ·ç­‰æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯
- **ğŸ“ˆ ç»¼åˆè¯„ä¼°**: å»ºç«‹å¤šç»´åº¦çš„æ€§èƒ½è¯„ä¼°ä½“ç³»
- ğŸ†• **å¯æ›¿æ¢ç‰¹å¾æå–å™¨**: æ”¯æŒå¤šç§å…ˆè¿›ç‰¹å¾æå–å™¨ï¼Œä¾¿äºå¯¹æ¯”ç ”ç©¶
- ğŸ†• **å®Œæ•´è®­ç»ƒè„šæœ¬**: æä¾›ä¾¿æ·çš„Shellè„šæœ¬ï¼Œç®€åŒ–è®­ç»ƒå’Œæµ‹è¯•æµç¨‹

### ğŸ¨ æŠ€æœ¯ç‰¹ç‚¹
- **åŒé›…å¯æ¯”è®¡ç®—**: æ•°å€¼é›…å¯æ¯”ï¼ˆåŸç‰ˆï¼‰vs è§£æé›…å¯æ¯”ï¼ˆæ”¹è¿›ç‰ˆï¼‰
- **çµæ´»è®­ç»ƒç­–ç•¥**: ä¸¤é˜¶æ®µè®­ç»ƒ vs ç«¯åˆ°ç«¯è®­ç»ƒ
- **æ™ºèƒ½ä½“ç´ åŒ–**: åŸºäºé‡å åŒºåŸŸçš„ä½“ç´ åŒ–å’Œé‡‡æ ·ç­–ç•¥
- **å¤šæ•°æ®é›†æ”¯æŒ**: ModelNet40ã€C3VDã€3DMatchã€KITTIç­‰
- **æ€§èƒ½åŸºå‡†æµ‹è¯•**: è¯¯å·®ã€é€Ÿåº¦ã€æ”¶æ•›æ€§ç­‰å¤šç»´åº¦è¯„ä¼°
- ğŸ†• **æ¨¡å—åŒ–ç‰¹å¾æå–**: æ”¯æŒPointNetã€AttentionNetã€CFormerã€FastAttentionã€Mamba3D
- ğŸ†• **ä½“ç´ åŒ–æ—¶æœºæ§åˆ¶**: æ”¯æŒå˜æ¢å‰/åä½“ç´ åŒ–ï¼Œé€‚åº”ä¸åŒåœºæ™¯éœ€æ±‚

---

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

### âœ… åŒæ¨¡å‹ç»Ÿä¸€æ”¯æŒ
- **åŸç‰ˆPointNetLK**: æ•°å€¼é›…å¯æ¯”è®¡ç®—ï¼Œä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå†…å­˜å‹å¥½
- **æ”¹è¿›ç‰ˆPointNetLK**: è§£æé›…å¯æ¯”è®¡ç®—ï¼Œç«¯åˆ°ç«¯è®­ç»ƒï¼Œç²¾åº¦æ›´é«˜
- **ç»Ÿä¸€æ¥å£**: é€šè¿‡æ¡¥æ¥æ¨¡å—æä¾›ä¸€è‡´çš„APIï¼Œæ— ç¼åˆ‡æ¢

### ğŸ†• å¯æ›¿æ¢ç‰¹å¾æå–å™¨æ”¯æŒ
- **PointNet**: åŸå§‹PointNetç‰¹å¾æå–å™¨ï¼ˆé»˜è®¤ï¼‰
- **AttentionNet**: åŸºäºå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾æå–å™¨
- **CFormer**: åŸºäºæ”¶é›†åˆ†å‘æœºåˆ¶çš„Transformerç‰¹å¾æå–å™¨
- **FastAttention**: è½»é‡çº§æ³¨æ„åŠ›ç‰¹å¾æå–å™¨ï¼Œå¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡
- **Mamba3D**: åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç‰¹å¾æå–å™¨ï¼Œå¤„ç†é•¿åºåˆ—ç‚¹äº‘
- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰ç‰¹å¾æå–å™¨å®ç°ç›¸åŒAPIï¼Œæ”¯æŒæ— ç¼æ›¿æ¢

### ğŸ¥ C3VDåŒ»å­¦æ•°æ®é›†å®Œæ•´æ”¯æŒ
- **å¤šé…å¯¹ç­–ç•¥**: ä¸€å¯¹ä¸€ã€åœºæ™¯å‚è€ƒã€æ•°æ®å¢å¼ºç­‰é…å¯¹æ–¹å¼
- **æ™ºèƒ½ä½“ç´ åŒ–**: åŸºäºPointNetLK_Revisitedçš„å…ˆè¿›ä½“ç´ åŒ–ç®—æ³•
- **ä½“ç´ åŒ–æ—¶æœºæ§åˆ¶**: æ”¯æŒå˜æ¢å‰/åä½“ç´ åŒ–ï¼Œé€‚åº”ä¸åŒå˜æ¢å¹…åº¦å’Œæ•°æ®è´¨é‡
- **ä¸“ç”¨è„šæœ¬**: `train_c3vd.sh`ã€`test_unified.py`ç­‰ä¸“é—¨çš„C3VDå¤„ç†è„šæœ¬
- **åŒ»å­¦ç‰¹åŒ–**: é’ˆå¯¹åŒ»å­¦å†…çª¥é•œæ•°æ®çš„ç‰¹æ®Šä¼˜åŒ–

### ğŸ”„ ç»Ÿä¸€è®­ç»ƒæµ‹è¯•æ¡†æ¶
- **ç»Ÿä¸€è®­ç»ƒè„šæœ¬** (`train_unified.py`): æ”¯æŒä¸¤ç§æ¨¡å‹å’Œå¤šç§ç‰¹å¾æå–å™¨çš„è®­ç»ƒ
- **ç»Ÿä¸€æµ‹è¯•è„šæœ¬** (`test_unified.py`): å•æ¨¡å‹æµ‹è¯•å’Œå¯¹æ¯”åˆ†æ
- **ç»¼åˆæµ‹è¯•è„šæœ¬** (`test_comprehensive.py`): é²æ£’æ€§å’Œç²¾åº¦çš„å…¨é¢è¯„ä¼°
- **æ‰¹é‡è®­ç»ƒè„šæœ¬** (`train_both_models.py`): åŒæ—¶è®­ç»ƒä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”
- **ä¾¿æ·Shellè„šæœ¬**: `train_c3vd.sh`ã€`train_modelnet.sh`ç­‰ä¸€é”®è®­ç»ƒè„šæœ¬

### ğŸ“Š æ€§èƒ½å¯¹æ¯”åˆ†æ
- **è¯¦ç»†å¯¹æ¯”æŠ¥å‘Š**: è¯¯å·®ã€é€Ÿåº¦ã€æ”¶æ•›æ€§ç­‰å¤šç»´åº¦åˆ†æ
- **é›…å¯æ¯”è®¡ç®—æ•ˆç‡**: æ•°å€¼vsè§£ææ–¹æ³•çš„æ€§èƒ½åŸºå‡†æµ‹è¯•
- **æ”¶æ•›è¡Œä¸ºåˆ†æ**: è¿­ä»£è¿‡ç¨‹å’Œæ”¶æ•›ç‰¹æ€§å¯¹æ¯”
- **é²æ£’æ€§è¯„ä¼°**: ç³»ç»Ÿæ€§æ‰°åŠ¨æµ‹è¯•ï¼Œè¯„ä¼°æ¨¡å‹åœ¨ä¸åŒè§’åº¦æ‰°åŠ¨ä¸‹çš„è¡¨ç°
- ğŸ†• **ç‰¹å¾æå–å™¨å¯¹æ¯”**: ä¸åŒç‰¹å¾æå–å™¨çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œåˆ†æ

### ğŸ”§ å¢å¼ºåŠŸèƒ½
- **ä½“ç´ åŒ–ä¼˜åŒ–**: æ™ºèƒ½ä½“ç´ åŒ–å’Œé‡‡æ ·ç­–ç•¥
- **å¤šé…å¯¹ç­–ç•¥**: æ”¯æŒå¤šç§æ•°æ®é…å¯¹å’Œå¢å¼ºæ–¹å¼
- **æ€§èƒ½ç›‘æ§**: è¯¦ç»†çš„è®­ç»ƒå’Œæµ‹è¯•æ—¥å¿—è®°å½•
- **å¯è§†åŒ–æ”¯æŒ**: é…å‡†ç»“æœå¯è§†åŒ–å’Œåˆ†æ
- ğŸ†• **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºæ‰©å±•çš„ç‰¹å¾æå–å™¨å’Œæ¨¡å‹æ¶æ„
- ğŸ†• **é…ç½®ç®¡ç†**: æ”¯æŒYAMLé…ç½®æ–‡ä»¶å’Œå‘½ä»¤è¡Œå‚æ•°

---

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

### ğŸ“ å®Œæ•´é¡¹ç›®ç»“æ„

```
PointNetLK_compare/
â”œâ”€â”€ README.md                      # é¡¹ç›®ä¸»æ–‡æ¡£
â”œâ”€â”€ README_C3VD.md                # C3VDæ•°æ®é›†ä¸“ç”¨æ–‡æ¡£
â”œâ”€â”€ TRAINING_GUIDE.md             # è¯¦ç»†è®­ç»ƒæŒ‡å—
â”œâ”€â”€ requirements.txt              # Pythonä¾èµ–åˆ—è¡¨
â”‚
â”œâ”€â”€ train_unified.py              # ç»Ÿä¸€è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_unified.py               # ç»Ÿä¸€æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_comprehensive.py         # ç»¼åˆå¯¹æ¯”æµ‹è¯•è„šæœ¬
â”œâ”€â”€ train_both_models.py          # åŒæ¨¡å‹è®­ç»ƒè„šæœ¬
â”‚
â”œâ”€â”€ model.py                      # æ”¹è¿›ç‰ˆPointNetLKæ¨¡å‹å®šä¹‰
â”œâ”€â”€ model_with_features.py        # æ”¯æŒç‰¹å¾æå–å™¨çš„æ”¹è¿›ç‰ˆæ¨¡å‹
â”œâ”€â”€ trainer.py                    # è®­ç»ƒå™¨ç±»
â”œâ”€â”€ utils.py                      # é€šç”¨å·¥å…·å‡½æ•°
â”œâ”€â”€ data_utils.py                 # æ•°æ®å¤„ç†å·¥å…·
â”‚
â”œâ”€â”€ feature_extractors/           # ğŸ†• ç‰¹å¾æå–å™¨æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py              # æ¨¡å—æ¥å£å®šä¹‰
â”‚   â”œâ”€â”€ base.py                  # åŸºç¡€ç‰¹å¾æå–å™¨æ¥å£
â”‚   â”œâ”€â”€ factory.py               # ç‰¹å¾æå–å™¨å·¥å‚ç±»
â”‚   â”œâ”€â”€ pointnet.py              # PointNetç‰¹å¾æå–å™¨
â”‚   â”œâ”€â”€ attention.py             # AttentionNetç‰¹å¾æå–å™¨
â”‚   â”œâ”€â”€ cformer.py               # CFormerç‰¹å¾æå–å™¨
â”‚   â”œâ”€â”€ fast_attention.py        # FastAttentionç‰¹å¾æå–å™¨
â”‚   â””â”€â”€ mamba3d.py               # Mamba3Dç‰¹å¾æå–å™¨
â”‚
â”œâ”€â”€ legacy_ptlk/                  # åŸç‰ˆPointNetLKå®ç°
â”‚   â”œâ”€â”€ pointlk.py               # åŸç‰ˆPointNetLKæ ¸å¿ƒ
â”‚   â”œâ”€â”€ pointlk_with_features.py # ğŸ†• æ”¯æŒç‰¹å¾æå–å™¨çš„åŸç‰ˆæ¨¡å‹
â”‚   â”œâ”€â”€ se3.py                   # SE3æç¾¤æ“ä½œ
â”‚   â”œâ”€â”€ so3.py                   # SO3æ—‹è½¬ç¾¤æ“ä½œ
â”‚   â””â”€â”€ invmat.py                # çŸ©é˜µæ±‚é€†å·¥å…·
â”‚
â”œâ”€â”€ bridge/                       # æ¡¥æ¥æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ unified_pointlk.py       # ç»Ÿä¸€æ¨¡å‹æ¥å£
â”‚
â”œâ”€â”€ comparison/                   # å¯¹æ¯”åˆ†ææ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_comparison.py       # æ¨¡å‹å¯¹æ¯”åˆ†æå™¨
â”‚
â”œâ”€â”€ config/                       # é…ç½®æ–‡ä»¶
â”œâ”€â”€ logs/                         # è®­ç»ƒæ—¥å¿—
â”œâ”€â”€ c3vd_results/                # C3VDè®­ç»ƒç»“æœ
â”œâ”€â”€ modelnet_results/            # ModelNet40ç»“æœ
â”œâ”€â”€ test_results_improved/       # æµ‹è¯•ç»“æœ
â”œâ”€â”€ experiments/                 # å®éªŒè®°å½•
â”œâ”€â”€ perturbation/               # æ‰°åŠ¨æµ‹è¯•æ•°æ®
â”‚
â”œâ”€â”€ train_c3vd.sh                # C3VDè®­ç»ƒä¾¿æ·è„šæœ¬
â”œâ”€â”€ train_modelnet.sh            # ModelNet40è®­ç»ƒè„šæœ¬
â”œâ”€â”€ quick_train.sh               # å¿«é€Ÿè®­ç»ƒè„šæœ¬
â”œâ”€â”€ run_comprehensive_test.sh    # ç»¼åˆæµ‹è¯•è„šæœ¬
â””â”€â”€ demo_comprehensive_test.sh   # æ¼”ç¤ºæµ‹è¯•è„šæœ¬
```

### ğŸ“‚ é‡è¦ç›®å½•è¯´æ˜

#### ğŸ—ï¸ æ ¸å¿ƒæ¨¡å—
- **`legacy_ptlk/`**: åŸç‰ˆPointNetLKçš„å®Œæ•´å®ç°ï¼ŒåŒ…å«æ‰€æœ‰æ•°å­¦å·¥å…·å’Œç®—æ³•
- **`model.py`**: æ”¹è¿›ç‰ˆPointNetLKæ¨¡å‹ï¼Œæ”¯æŒè§£æé›…å¯æ¯”è®¡ç®—
- **`model_with_features.py`**: ğŸ†• æ”¯æŒå¯æ›¿æ¢ç‰¹å¾æå–å™¨çš„æ”¹è¿›ç‰ˆæ¨¡å‹
- **`data_utils.py`**: ç»Ÿä¸€çš„æ•°æ®å¤„ç†å·¥å…·ï¼Œæ”¯æŒå¤šç§æ•°æ®é›†æ ¼å¼

#### ğŸ†• ç‰¹å¾æå–å™¨æ¨¡å—
- **`feature_extractors/`**: å¯æ›¿æ¢ç‰¹å¾æå–å™¨çš„å®Œæ•´å®ç°
  - **`base.py`**: å®šä¹‰BaseFeatureExtractoræŠ½è±¡åŸºç±»
  - **`factory.py`**: ç‰¹å¾æå–å™¨å·¥å‚ï¼Œæ”¯æŒåŠ¨æ€åˆ›å»º
  - **å„ç§ç‰¹å¾æå–å™¨**: PointNetã€AttentionNetã€CFormerã€FastAttentionã€Mamba3D

#### ğŸŒ‰ ç»Ÿä¸€æ¥å£
- **`bridge/`**: æä¾›ç»Ÿä¸€çš„APIæ¥å£ï¼Œå®ç°ä¸¤ä¸ªç‰ˆæœ¬çš„æ— ç¼åˆ‡æ¢
- **`comparison/`**: æ€§èƒ½å¯¹æ¯”åˆ†æå·¥å…·

#### ğŸ“Š ç»“æœç®¡ç†
- **`c3vd_results/`**: C3VDæ•°æ®é›†çš„è®­ç»ƒç»“æœå’Œæ—¥å¿—
- **`modelnet_results/`**: ModelNet40æ•°æ®é›†çš„è®­ç»ƒç»“æœ
- **`logs/`**: è¯¦ç»†çš„è®­ç»ƒå’Œæµ‹è¯•æ—¥å¿—
- **`experiments/`**: å®éªŒé…ç½®å’Œç»“æœè®°å½•

---

## ğŸ”§ ç¯å¢ƒé…ç½®

### åŸºç¡€ç¯å¢ƒè¦æ±‚

```bash
# Pythonç‰ˆæœ¬è¦æ±‚
Python >= 3.7

# æ“ä½œç³»ç»Ÿæ”¯æŒ
- Linux (æ¨è)
- Windows (WSLæ¨è)
- macOS
```

### ä¾èµ–å®‰è£…

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n pointnetlk python=3.8
conda activate pointnetlk

# å®‰è£…PyTorch (æ ¹æ®æ‚¨çš„CUDAç‰ˆæœ¬)
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CPUç‰ˆæœ¬
# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# å®‰è£…é¡¹ç›®ä¾èµ–
pip install -r requirements.txt

# å®‰è£…é¢å¤–ä¾èµ–
pip install matplotlib seaborn  # å¯è§†åŒ–
pip install jupyter             # Jupyteræ”¯æŒ
```

### è¯¦ç»†ä¾èµ–åˆ—è¡¨

```bash
# å¿…éœ€ä¾èµ–
numpy>=1.19.0
scipy>=1.5.0
open3d>=0.13.0
h5py>=2.10.0
six>=1.15.0
tqdm>=4.60.0

# å¯é€‰ä¾èµ–
matplotlib>=3.3.0    # å¯è§†åŒ–
seaborn>=0.11.0      # ç»Ÿè®¡å›¾è¡¨
jupyter>=1.0.0       # Notebookæ”¯æŒ
tensorboard>=2.4.0   # è®­ç»ƒç›‘æ§
```

### CUDAé…ç½®éªŒè¯

```python
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    print(f"å½“å‰GPU: {torch.cuda.get_device_name(0)}")
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. é¡¹ç›®å…‹éš†å’Œç¯å¢ƒé…ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd PointNetLK_compare

# é…ç½®ç¯å¢ƒ
conda create -n pointnetlk python=3.8
conda activate pointnetlk
pip install -r requirements.txt
```

### 2. å¿«é€Ÿæ¼”ç¤º

```bash
# ä½¿ç”¨æ¼”ç¤ºæ•°æ®è¿›è¡Œå¿«é€Ÿæµ‹è¯•
python test_unified.py \
    --dataset demo \
    --model_type improved \
    --output_dir ./quick_demo

# æŸ¥çœ‹ç»“æœ
cat ./quick_demo/test_results.txt
```

### 3. ModelNet40å¿«é€Ÿè®­ç»ƒ

```bash
# å‡†å¤‡ModelNet40æ•°æ®é›†
mkdir -p dataset
cd dataset
wget https://shapenet.cs.princeton.edu/media/modelnet40_ply_hdf5_2048.zip
unzip modelnet40_ply_hdf5_2048.zip
ln -s modelnet40_ply_hdf5_2048 ModelNet40
cd ..

# å¿«é€Ÿè®­ç»ƒï¼ˆ2ä¸ªepochï¼‰
python train_unified.py \
    --dataset modelnet \
    --data_root ./dataset/ModelNet40 \
    --model_type improved \
    --epochs 2 \
    --batch_size 16 \
    --output_prefix ./quick_train

# æµ‹è¯•è®­ç»ƒç»“æœ
python test_unified.py \
    --dataset modelnet \
    --data_root ./dataset/ModelNet40 \
    --model_path ./quick_train_best.pth \
    --model_type improved
```

### 4. C3VDæ•°æ®é›†å¿«é€Ÿå¼€å§‹

```bash
# å‡è®¾æ‚¨å·²ç»æœ‰C3VDæ•°æ®é›†
export C3VD_ROOT=/path/to/C3VD_sever_datasets

# å¿«é€Ÿè®­ç»ƒ
python train_c3vd.py \
    --c3vd-root $C3VD_ROOT \
    --output-prefix ./c3vd_quick \
    --epochs 10 \
    --batch-size 8

# å¿«é€Ÿæµ‹è¯•
python test_c3vd.py \
    --c3vd-root $C3VD_ROOT \
    --model-path ./c3vd_quick_best.pth \
    --output-dir ./c3vd_test_results
```

---

## ğŸ“ æ•°æ®é›†æ”¯æŒ

### æ”¯æŒçš„æ•°æ®é›†

| æ•°æ®é›† | çŠ¶æ€ | ç”¨é€” | ä¸“ç”¨è„šæœ¬ |
|--------|------|------|----------|
| **C3VD** | âœ… å®Œæ•´æ”¯æŒ | åŒ»å­¦å†…çª¥é•œç‚¹äº‘é…å‡† | `train_unified.py --dataset-type c3vd`, `test_unified.py --dataset-type c3vd` |
| **ModelNet40** | âœ… å®Œæ•´æ”¯æŒ | æ ‡å‡†3Då½¢çŠ¶é…å‡†åŸºå‡† | `train_unified.py`, `test_unified.py` |
| **æ¼”ç¤ºæ•°æ®** | âœ… å†…ç½® | å¿«é€Ÿæµ‹è¯•å’Œæ¼”ç¤º | æ‰€æœ‰è„šæœ¬ |
| **3DMatch** | ğŸ”„ éƒ¨åˆ†æ”¯æŒ | å®¤å†…åœºæ™¯é…å‡† | `train_unified.py` |
| **KITTI** | ğŸ”„ éƒ¨åˆ†æ”¯æŒ | è‡ªåŠ¨é©¾é©¶ç‚¹äº‘é…å‡† | `train_unified.py` |

### C3VDæ•°æ®é›†é…ç½®

C3VDæ•°æ®é›†æ˜¯æœ¬é¡¹ç›®çš„é‡ç‚¹æ”¯æŒæ•°æ®é›†ï¼Œè¯·å‚è€ƒ[README_C3VD.md](README_C3VD.md)è·å–è¯¦ç»†ä¿¡æ¯ï¼š

```bash
# æ•°æ®é›†ç»“æ„
C3VD_sever_datasets/
â”œâ”€â”€ C3VD_ply_source/              # æºç‚¹äº‘ï¼ˆæ·±åº¦ä¼ æ„Ÿå™¨ï¼‰
â”œâ”€â”€ visible_point_cloud_ply_depth/ # ç›®æ ‡ç‚¹äº‘ï¼ˆå¯è§ç‚¹äº‘ï¼‰
â””â”€â”€ C3VD_ref/                     # å‚è€ƒç‚¹äº‘
```

### ModelNet40æ•°æ®é›†é…ç½®

```bash
# ä¸‹è½½ModelNet40
cd dataset
wget https://shapenet.cs.princeton.edu/media/modelnet40_ply_hdf5_2048.zip
unzip modelnet40_ply_hdf5_2048.zip
ln -s modelnet40_ply_hdf5_2048 ModelNet40

# éªŒè¯æ•°æ®é›†
python -c "
import os
print(f'ModelNet40å­˜åœ¨: {os.path.exists(\"dataset/ModelNet40\")}')
print(f'è®­ç»ƒæ•°æ®: {os.path.exists(\"dataset/ModelNet40/train_files.txt\")}')
print(f'æµ‹è¯•æ•°æ®: {os.path.exists(\"dataset/ModelNet40/test_files.txt\")}')
"
```

---

## ğŸ§  ç‰¹å¾æå–å™¨

æœ¬é¡¹ç›®æ”¯æŒå¤šç§å…ˆè¿›çš„ç‰¹å¾æå–å™¨ï¼Œä¸ºPointNetLKæä¾›æ›´å¼ºçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ï¼š

### æ”¯æŒçš„ç‰¹å¾æå–å™¨

| ç‰¹å¾æå–å™¨ | ç±»å‹ | æ ¸å¿ƒæŠ€æœ¯ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------------|------|----------|------|----------|
| **PointNet** | ç»å…¸ | MLP + Maxæ± åŒ– | ç®€å•é«˜æ•ˆã€å†…å­˜å‹å¥½ | åŸºå‡†æµ‹è¯•ã€å¿«é€ŸåŸå‹ |
| **AttentionNet** | æ³¨æ„åŠ› | å¤šå¤´è‡ªæ³¨æ„åŠ› | æ•è·å…¨å±€ä¾èµ–å…³ç³» | å¤æ‚åœºæ™¯ã€é«˜ç²¾åº¦è¦æ±‚ |
| **CFormer** | Transformer | æ”¶é›†åˆ†å‘æœºåˆ¶ | é«˜æ•ˆTransformer | å¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡ |
| **FastAttention** | è½»é‡æ³¨æ„åŠ› | ç®€åŒ–æ³¨æ„åŠ›æœºåˆ¶ | å¿«é€Ÿæ¨ç† | å®æ—¶åº”ç”¨ã€èµ„æºå—é™ |
| **Mamba3D** | çŠ¶æ€ç©ºé—´ | é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ | é•¿åºåˆ—å»ºæ¨¡ | å¤§è§„æ¨¡ç‚¹äº‘ã€æ—¶åºæ•°æ® |

### ç‰¹å¾æå–å™¨è¯¦ç»†è¯´æ˜

#### PointNet
```python
# ç»å…¸PointNetç‰¹å¾æå–å™¨
python train_unified.py \
    --dataset-type c3vd \
    --feature-extractor pointnet \
    --epochs 100
```
- **ä¼˜ç‚¹**: ç®€å•é«˜æ•ˆï¼Œå†…å­˜å ç”¨ä½ï¼Œè®­ç»ƒç¨³å®š
- **ç¼ºç‚¹**: ç‰¹å¾è¡¨è¾¾èƒ½åŠ›æœ‰é™
- **æ¨èåœºæ™¯**: åŸºå‡†æµ‹è¯•ã€å¿«é€ŸéªŒè¯ã€èµ„æºå—é™ç¯å¢ƒ

#### AttentionNet
```python
# åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç‰¹å¾æå–å™¨
python train_unified.py \
    --dataset-type c3vd \
    --feature-extractor attention \
    --attention-heads 8 \
    --attention-blocks 4 \
    --epochs 100
```
- **ä¼˜ç‚¹**: å¼ºå¤§çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼Œå¯è§£é‡Šæ€§å¥½
- **ç¼ºç‚¹**: è®¡ç®—å¤æ‚åº¦é«˜ï¼Œå†…å­˜å ç”¨å¤§
- **æ¨èåœºæ™¯**: é«˜ç²¾åº¦è¦æ±‚ã€å¤æ‚åœºæ™¯é…å‡†

#### CFormer
```python
# åŸºäºæ”¶é›†åˆ†å‘æœºåˆ¶çš„Transformer
python train_unified.py \
    --dataset-type c3vd \
    --feature-extractor cformer \
    --cformer-dim 512 \
    --cformer-heads 8 \
    --epochs 100
```
- **ä¼˜ç‚¹**: é«˜æ•ˆçš„Transformeræ¶æ„ï¼Œå¹³è¡¡æ€§èƒ½ä¸æ•ˆç‡
- **ç¼ºç‚¹**: å‚æ•°è¾ƒå¤šï¼Œéœ€è¦è¶³å¤Ÿè®­ç»ƒæ•°æ®
- **æ¨èåœºæ™¯**: å¤§è§„æ¨¡æ•°æ®é›†ã€é«˜æ€§èƒ½è¦æ±‚

#### FastAttention
```python
# è½»é‡çº§æ³¨æ„åŠ›ç‰¹å¾æå–å™¨
python train_unified.py \
    --dataset-type c3vd \
    --feature-extractor fast_attention \
    --fast-attention-dim 256 \
    --epochs 100
```
- **ä¼˜ç‚¹**: å¿«é€Ÿæ¨ç†ï¼Œå†…å­˜å‹å¥½ï¼Œä¿æŒæ³¨æ„åŠ›ä¼˜åŠ¿
- **ç¼ºç‚¹**: ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ç•¥ä½äºå®Œæ•´æ³¨æ„åŠ›
- **æ¨èåœºæ™¯**: å®æ—¶åº”ç”¨ã€è¾¹ç¼˜è®¾å¤‡ã€å¿«é€ŸåŸå‹

#### Mamba3D
```python
# åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç‰¹å¾æå–å™¨
python train_unified.py \
    --dataset-type c3vd \
    --feature-extractor mamba3d \
    --mamba-layers 6 \
    --mamba-dim 512 \
    --epochs 100
```
- **ä¼˜ç‚¹**: ä¼˜ç§€çš„é•¿åºåˆ—å»ºæ¨¡èƒ½åŠ›ï¼Œçº¿æ€§å¤æ‚åº¦
- **ç¼ºç‚¹**: æ–°å…´æ¶æ„ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè°ƒä¼˜
- **æ¨èåœºæ™¯**: å¤§è§„æ¨¡ç‚¹äº‘ã€æ—¶åºç‚¹äº‘æ•°æ®

### ç‰¹å¾æå–å™¨æ€§èƒ½å¯¹æ¯”

åŸºäºC3VDæ•°æ®é›†çš„åˆæ­¥æµ‹è¯•ç»“æœï¼š

| ç‰¹å¾æå–å™¨ | æ—‹è½¬è¯¯å·®(Â°) | å¹³ç§»è¯¯å·® | æ¨ç†æ—¶é—´(ms) | GPUå†…å­˜(MB) | è®­ç»ƒç¨³å®šæ€§ |
|------------|-------------|----------|--------------|-------------|------------|
| PointNet | 2.8 | 0.052 | 15 | 1200 | â­â­â­â­â­ |
| AttentionNet | 2.1 | 0.041 | 45 | 3200 | â­â­â­â­ |
| CFormer | 2.3 | 0.045 | 35 | 2800 | â­â­â­â­ |
| FastAttention | 2.5 | 0.048 | 22 | 1800 | â­â­â­â­â­ |
| Mamba3D | 2.4 | 0.046 | 28 | 2200 | â­â­â­â­ |

### ä½¿ç”¨ä¾¿æ·è„šæœ¬

é¡¹ç›®æä¾›äº†ä¾¿æ·çš„è®­ç»ƒè„šæœ¬ï¼Œæ”¯æŒç‰¹å¾æå–å™¨é€‰æ‹©ï¼š

```bash
# ä½¿ç”¨CFormerç‰¹å¾æå–å™¨è®­ç»ƒ
./train_c3vd.sh -f cformer -e 50 -b 8

# ä½¿ç”¨AttentionNetç‰¹å¾æå–å™¨è®­ç»ƒ
./train_c3vd.sh -f attention -e 100 -b 4

# ä½¿ç”¨Mamba3Dç‰¹å¾æå–å™¨è®­ç»ƒ
./train_c3vd.sh -f mamba3d -e 75 -b 6
```

### è‡ªå®šä¹‰ç‰¹å¾æå–å™¨

é¡¹ç›®æ”¯æŒè½»æ¾æ‰©å±•æ–°çš„ç‰¹å¾æå–å™¨ï¼š

```python
# 1. ç»§æ‰¿BaseFeatureExtractor
from feature_extractors.base import BaseFeatureExtractor

class CustomFeatureExtractor(BaseFeatureExtractor):
    def __init__(self, dim_k=1024):
        super().__init__(dim_k)
        # è‡ªå®šä¹‰ç½‘ç»œç»“æ„
        
    def forward(self, points):
        # å®ç°ç‰¹å¾æå–é€»è¾‘
        # è¾“å…¥: [B, N, 3] -> è¾“å‡º: [B, dim_k]
        pass

# 2. æ³¨å†Œåˆ°å·¥å‚
from feature_extractors.factory import FeatureExtractorFactory
FeatureExtractorFactory.register('custom', CustomFeatureExtractor)

# 3. ä½¿ç”¨æ–°ç‰¹å¾æå–å™¨
python train_unified.py --feature-extractor custom
```

---

## ğŸ“ è®­ç»ƒæŒ‡å—

### C3VDæ•°æ®é›†è®­ç»ƒ

#### åŸºç¡€è®­ç»ƒ
```bash
# æ”¹è¿›ç‰ˆPointNetLKè®­ç»ƒï¼ˆé»˜è®¤PointNetç‰¹å¾æå–å™¨ï¼‰
python train_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --outfile ./c3vd_results/basic \
    --model-type improved \
    --epochs 100 \
    --batch-size 16

# åŸç‰ˆPointNetLKè®­ç»ƒ
python train_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --outfile ./c3vd_results/original \
    --model-type original \
    --epochs 100 \
    --batch-size 16
```

#### ğŸ†• ä½¿ç”¨ä¸åŒç‰¹å¾æå–å™¨è®­ç»ƒ
```bash
# ä½¿ç”¨AttentionNetç‰¹å¾æå–å™¨
python train_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --outfile ./c3vd_results/attention \
    --model-type improved \
    --feature-extractor attention \
    --attention-heads 8 \
    --attention-blocks 4 \
    --epochs 100

# ä½¿ç”¨CFormerç‰¹å¾æå–å™¨
python train_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --outfile ./c3vd_results/cformer \
    --model-type improved \
    --feature-extractor cformer \
    --cformer-dim 512 \
    --epochs 100

# ä½¿ç”¨Mamba3Dç‰¹å¾æå–å™¨
python train_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --outfile ./c3vd_results/mamba3d \
    --model-type improved \
    --feature-extractor mamba3d \
    --mamba-layers 6 \
    --epochs 100
```

#### ğŸ†• ä¾¿æ·Shellè„šæœ¬è®­ç»ƒ
```bash
# è®¾ç½®æ‰§è¡Œæƒé™
chmod +x train_c3vd.sh

# ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒï¼ˆPointNetï¼‰
./train_c3vd.sh

# ä½¿ç”¨CFormerç‰¹å¾æå–å™¨è®­ç»ƒ
./train_c3vd.sh -f cformer -e 50 -b 8

# ä½¿ç”¨AttentionNetç‰¹å¾æå–å™¨è®­ç»ƒ
./train_c3vd.sh -f attention -e 100 -b 4 --voxel-size 0.03

# å¿«é€Ÿæµ‹è¯•æ¨¡å¼
./train_c3vd.sh --quick-test -f fast_attention

# ä½¿ç”¨åœºæ™¯åˆ’åˆ†è®­ç»ƒ
./train_c3vd.sh --scene-split --split-ratio 0.8 -f mamba3d
```

#### é«˜çº§é…ç½®è®­ç»ƒ
```bash
# ä½¿ç”¨åœºæ™¯å‚è€ƒé…å¯¹ç­–ç•¥
python train_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --outfile ./c3vd_results/advanced \
    --model-type improved \
    --c3vd-pairing-strategy all \
    --c3vd-transform-mag 0.6 \
    --voxel-grid-size 64 \
    --epochs 200

# ä½“ç´ åŒ–æ—¶æœºæ§åˆ¶
# å˜æ¢åä½“ç´ åŒ–ï¼ˆé»˜è®¤ï¼Œé€‚åˆæ ‡å‡†è®­ç»ƒï¼‰
python train_unified.py \
    --dataset-type c3vd \
    --voxel-after-transf \
    --c3vd-transform-mag 0.8

# å˜æ¢å‰ä½“ç´ åŒ–ï¼ˆé€‚åˆå¤§å¹…åº¦å˜æ¢ï¼‰
python train_unified.py \
    --dataset-type c3vd \
    --voxel-before-transf \
    --c3vd-transform-mag 1.0
```

### ModelNet40æ•°æ®é›†è®­ç»ƒ

#### ç»Ÿä¸€è®­ç»ƒè„šæœ¬
```bash
# æ”¹è¿›ç‰ˆPointNetLKï¼ˆé»˜è®¤PointNetï¼‰
python train_unified.py \
    --dataset modelnet \
    --data_root ./dataset/ModelNet40 \
    --model_type improved \
    --epochs 50 \
    --batch_size 32 \
    --output_prefix ./modelnet_improved

# ä½¿ç”¨AttentionNetç‰¹å¾æå–å™¨
python train_unified.py \
    --dataset modelnet \
    --data_root ./dataset/ModelNet40 \
    --model_type improved \
    --feature-extractor attention \
    --epochs 50 \
    --batch_size 16 \
    --output_prefix ./modelnet_attention

# åŸç‰ˆPointNetLK
python train_unified.py \
    --dataset modelnet \
    --data_root ./dataset/ModelNet40 \
    --model_type original \
    --epochs 50 \
    --batch_size 32 \
    --output_prefix ./modelnet_original
```

#### ğŸ†• ä¾¿æ·Shellè„šæœ¬è®­ç»ƒ
```bash
# è®¾ç½®æ‰§è¡Œæƒé™
chmod +x train_modelnet.sh

# ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
./train_modelnet.sh

# ä½¿ç”¨CFormerç‰¹å¾æå–å™¨è®­ç»ƒ
./train_modelnet.sh -f cformer -e 75 -b 16

# ä½¿ç”¨ç‰¹å®šç±»åˆ«è®­ç»ƒ
./train_modelnet.sh -c airplane -f attention -e 100
```

#### å¯¹æ¯”è®­ç»ƒ
```bash
# åŒæ—¶è®­ç»ƒä¸¤ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”
python train_both_models.py \
    --data_root ./dataset/ModelNet40 \
    --epochs 20 \
    --batch_size 16 \
    --output_prefix ./modelnet_comparison

# ä¸åŒç‰¹å¾æå–å™¨å¯¹æ¯”è®­ç»ƒ
python train_both_models.py \
    --data_root ./dataset/ModelNet40 \
    --feature_extractor_1 pointnet \
    --feature_extractor_2 cformer \
    --epochs 30 \
    --output_prefix ./feature_comparison
```

### ğŸ†• ç‰¹å¾æå–å™¨ç‰¹å®šå‚æ•°

#### AttentionNetå‚æ•°
```bash
--attention-heads 8           # æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤8ï¼‰
--attention-blocks 4          # æ³¨æ„åŠ›å—æ•°ï¼ˆé»˜è®¤4ï¼‰
--attention-dim 512          # æ³¨æ„åŠ›ç»´åº¦ï¼ˆé»˜è®¤512ï¼‰
--attention-dropout 0.1      # Dropoutç‡ï¼ˆé»˜è®¤0.1ï¼‰
```

#### CFormerå‚æ•°
```bash
--cformer-dim 512            # CFormerç»´åº¦ï¼ˆé»˜è®¤512ï¼‰
--cformer-heads 8            # æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤8ï¼‰
--cformer-blocks 6           # Transformerå—æ•°ï¼ˆé»˜è®¤6ï¼‰
--cformer-dropout 0.1        # Dropoutç‡ï¼ˆé»˜è®¤0.1ï¼‰
```

#### Mamba3Då‚æ•°
```bash
--mamba-layers 6             # Mambaå±‚æ•°ï¼ˆé»˜è®¤6ï¼‰
--mamba-dim 512             # çŠ¶æ€ç»´åº¦ï¼ˆé»˜è®¤512ï¼‰
--mamba-expand 2            # æ‰©å±•å› å­ï¼ˆé»˜è®¤2ï¼‰
```

#### FastAttentionå‚æ•°
```bash
--fast-attention-dim 256     # ç‰¹å¾ç»´åº¦ï¼ˆé»˜è®¤256ï¼‰
--fast-attention-heads 4     # æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤4ï¼‰
```

### è®­ç»ƒç›‘æ§å’Œæ—¥å¿—

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python train_unified.py \
    --dataset-type c3vd \
    --verbose \
    --log-interval 10

# ä½¿ç”¨TensorBoardç›‘æ§ï¼ˆå¦‚æœæ”¯æŒï¼‰
tensorboard --logdir ./logs

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f ./logs/train.log
```

---

## ğŸ§ª æµ‹è¯•æŒ‡å—

### å•æ¨¡å‹æµ‹è¯•

#### C3VDæ•°æ®é›†æµ‹è¯•
```bash
# åŸºç¡€æµ‹è¯•
python test_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --model-type improved \
    --model-path ./c3vd_results/model.pth

# ğŸ†• æµ‹è¯•ä¸åŒç‰¹å¾æå–å™¨
python test_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --model-type improved \
    --model-path ./c3vd_results/cformer_model.pth \
    --feature-extractor cformer

# æµ‹è¯•åŸç‰ˆPointNetLK
python test_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --model-type original \
    --model-path ./c3vd_results/original_model.pth
```

#### ModelNet40æ•°æ®é›†æµ‹è¯•
```bash
# æ”¹è¿›ç‰ˆPointNetLKæµ‹è¯•
python test_unified.py \
    --dataset modelnet \
    --data_root ./dataset/ModelNet40 \
    --model_type improved \
    --pretrained ./modelnet_results/improved_model.pth

# ğŸ†• ä½¿ç”¨AttentionNetç‰¹å¾æå–å™¨æµ‹è¯•
python test_unified.py \
    --dataset modelnet \
    --data_root ./dataset/ModelNet40 \
    --model_type improved \
    --feature-extractor attention \
    --pretrained ./modelnet_results/attention_model.pth

# åŸç‰ˆPointNetLKæµ‹è¯•
python test_unified.py \
    --dataset modelnet \
    --data_root ./dataset/ModelNet40 \
    --model_type original \
    --pretrained ./modelnet_results/original_model.pth
```

### ğŸ†• ç»¼åˆå¯¹æ¯”æµ‹è¯•

#### å®Œæ•´æ€§èƒ½å¯¹æ¯”
```bash
# ä¸¤ç§æ¨¡å‹å®Œæ•´å¯¹æ¯”
python test_comprehensive.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --improved-model ./c3vd_results/improved_model.pth \
    --original-model ./c3vd_results/original_model.pth \
    --output-dir ./test_results_improved/comprehensive

# ğŸ†• å¤šç‰¹å¾æå–å™¨å¯¹æ¯”
python test_comprehensive.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --models-config feature_comparison.yaml \
    --output-dir ./test_results_improved/feature_comparison
```

#### ä½¿ç”¨ä¾¿æ·è„šæœ¬æµ‹è¯•
```bash
# è®¾ç½®æ‰§è¡Œæƒé™
chmod +x run_comprehensive_test.sh

# è¿è¡Œå®Œæ•´å¯¹æ¯”æµ‹è¯•
./run_comprehensive_test.sh

# å¿«é€Ÿæ¼”ç¤ºæµ‹è¯•
./demo_comprehensive_test.sh

# è‡ªå®šä¹‰æµ‹è¯•é…ç½®
./run_comprehensive_test.sh \
    --models ./models_config.yaml \
    --dataset c3vd \
    --output ./custom_test_results
```

### é²æ£’æ€§æµ‹è¯•

#### ğŸ†• ä¸åŒæ‰°åŠ¨è§’åº¦æµ‹è¯•
```bash
# æµ‹è¯•ä¸åŒè§’åº¦æ‰°åŠ¨ä¸‹çš„æ€§èƒ½
python test_unified.py \
    --dataset-type c3vd \
    --dataset-path /path/to/C3VD_datasets \
    --model-type improved \
    --model-path ./c3vd_results/model.pth \
    --perturbation-angles 5 10 15 20 30 45 60 \
    --output-dir ./test_results_improved/robustness

# ğŸ†• ç‰¹å¾æå–å™¨é²æ£’æ€§å¯¹æ¯”
python test_unified.py \
    --dataset-type c3vd \
    --robustness-test \
    --feature-extractors pointnet attention cformer mamba3d \
    --perturbation-range 0-60 \
    --output-dir ./test_results_improved/feature_robustness
```

#### ç³»ç»Ÿæ€§æ‰°åŠ¨æµ‹è¯•
```bash
# ç”Ÿæˆæ‰°åŠ¨æµ‹è¯•æ•°æ®
python test_unified.py \
    --dataset-type c3vd \
    --generate-perturbations \
    --perturbation-output ./perturbation/

# è¿è¡Œç³»ç»Ÿæ€§æµ‹è¯•
python test_unified.py \
    --dataset-type c3vd \
    --perturbation-data ./perturbation/ \
    --model-path ./c3vd_results/model.pth \
    --systematic-test
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

#### é€Ÿåº¦å’Œå†…å­˜æµ‹è¯•
```bash
# ğŸ†• ä¸åŒç‰¹å¾æå–å™¨æ€§èƒ½åŸºå‡†
python test_unified.py \
    --benchmark-mode \
    --feature-extractors pointnet attention cformer fast_attention mamba3d \
    --benchmark-iterations 100 \
    --output-dir ./benchmarks/

# å†…å­˜ä½¿ç”¨åˆ†æ
python test_unified.py \
    --memory-profile \
    --model-path ./c3vd_results/model.pth \
    --feature-extractor cformer
```

#### æ”¶æ•›æ€§åˆ†æ
```bash
# è¿­ä»£æ”¶æ•›åˆ†æ
python test_unified.py \
    --convergence-analysis \
    --model-path ./c3vd_results/model.pth \
    --max-iterations 50 \
    --output-dir ./analysis/convergence
```

### å¯è§†åŒ–æµ‹è¯•

#### é…å‡†ç»“æœå¯è§†åŒ–
```bash
# ç”Ÿæˆé…å‡†ç»“æœå¯è§†åŒ–
python test_unified.py \
    --dataset-type c3vd \
    --model-path ./c3vd_results/model.pth \
    --visualize \
    --num-samples 10 \
    --output-dir ./visualizations/

# ğŸ†• ç‰¹å¾æå–å™¨å¯¹æ¯”å¯è§†åŒ–
python test_unified.py \
    --feature-comparison-viz \
    --models-config ./configs/feature_comparison.yaml \
    --output-dir ./visualizations/feature_comparison
```

### è¯¦ç»†æµ‹è¯•æŠ¥å‘Š

#### ç”Ÿæˆå®Œæ•´æµ‹è¯•æŠ¥å‘Š
```bash
# å®Œæ•´æµ‹è¯•æŠ¥å‘Š
python test_unified.py \
    --dataset-type c3vd \
    --model-path ./c3vd_results/model.pth \
    --detailed-report \
    --report-format html \
    --output-dir ./reports/

# ğŸ†• ç‰¹å¾æå–å™¨å¯¹æ¯”æŠ¥å‘Š
python test_unified.py \
    --feature-extractor-comparison \
    --models-dir ./c3vd_results/ \
    --report-format pdf \
    --output-dir ./reports/feature_comparison
```

### æµ‹è¯•é…ç½®æ–‡ä»¶

#### ç¤ºä¾‹é…ç½®æ–‡ä»¶ `test_config.yaml`
```yaml
# æµ‹è¯•é…ç½®ç¤ºä¾‹
dataset:
  type: c3vd
  path: /path/to/C3VD_datasets
  
models:
  - name: "improved_pointnet"
    type: improved
    path: "./c3vd_results/pointnet_model.pth"
    feature_extractor: pointnet
    
  - name: "improved_cformer"
    type: improved  
    path: "./c3vd_results/cformer_model.pth"
    feature_extractor: cformer
    
  - name: "original_pointnet"
    type: original
    path: "./c3vd_results/original_model.pth"

test_settings:
  perturbation_angles: [5, 10, 15, 20, 30, 45, 60]
  num_samples: 100
  generate_visualization: true
  detailed_report: true
```

#### ä½¿ç”¨é…ç½®æ–‡ä»¶æµ‹è¯•
```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶è¿›è¡Œæµ‹è¯•
python test_unified.py --config test_config.yaml

# ğŸ†• æ‰¹é‡ç‰¹å¾æå–å™¨æµ‹è¯•
python test_unified.py --config feature_extractors_test.yaml
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### ç®—æ³•æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | åŸç‰ˆPointNetLK | æ”¹è¿›ç‰ˆPointNetLK | æ”¹è¿›å¹…åº¦ |
|------|----------------|-------------------|----------|
| **é›…å¯æ¯”è®¡ç®—** | æ•°å€¼å¾®åˆ† | è§£ææ±‚å¯¼ | ç²¾åº¦æå‡ |
| **è®­ç»ƒç­–ç•¥** | ä¸¤é˜¶æ®µè®­ç»ƒ | ç«¯åˆ°ç«¯è®­ç»ƒ | ç®€åŒ–æµç¨‹ |
| **å†…å­˜ä½¿ç”¨** | ä½ | ä¸­ç­‰ | å¯æ¥å— |
| **æ¨ç†é€Ÿåº¦** | 0.086s | 0.049s | 1.76xåŠ é€Ÿ |
| **é…å‡†ç²¾åº¦** | 30.72Â° | 30.35Â° | 1.2%æå‡ |

### C3VDæ•°æ®é›†æ€§èƒ½

åŸºäºC3VDæ•°æ®é›†çš„æµ‹è¯•ç»“æœï¼š

| é…å¯¹ç­–ç•¥ | åŸç‰ˆPointNetLK | æ”¹è¿›ç‰ˆPointNetLK |
|----------|----------------|-------------------|
| **ä¸€å¯¹ä¸€é…å¯¹** | æ—‹è½¬è¯¯å·®: 3.2Â°<br>å¹³ç§»è¯¯å·®: 0.067 | æ—‹è½¬è¯¯å·®: 2.8Â°<br>å¹³ç§»è¯¯å·®: 0.052 |
| **åœºæ™¯å‚è€ƒ** | æ—‹è½¬è¯¯å·®: 4.1Â°<br>å¹³ç§»è¯¯å·®: 0.089 | æ—‹è½¬è¯¯å·®: 3.5Â°<br>å¹³ç§»è¯¯å·®: 0.073 |
| **æ•°æ®å¢å¼º** | æ—‹è½¬è¯¯å·®: 2.9Â°<br>å¹³ç§»è¯¯å·®: 0.058 | æ—‹è½¬è¯¯å·®: 2.4Â°<br>å¹³ç§»è¯¯å·®: 0.045 |

### ModelNet40åŸºå‡†æµ‹è¯•

| æµ‹è¯•åœºæ™¯ | åŸç‰ˆPointNetLK | æ”¹è¿›ç‰ˆPointNetLK |
|----------|----------------|-------------------|
| **æ ‡å‡†æµ‹è¯•** | å¹³å‡è¯¯å·®: 30.72Â° | å¹³å‡è¯¯å·®: 30.35Â° |
| **å™ªå£°æµ‹è¯•** | å¹³å‡è¯¯å·®: 35.48Â° | å¹³å‡è¯¯å·®: 34.12Â° |
| **éƒ¨åˆ†é®æŒ¡** | å¹³å‡è¯¯å·®: 42.15Â° | å¹³å‡è¯¯å·®: 39.87Â° |

---

## ğŸ”Œ APIä½¿ç”¨æŒ‡å—

### åŸºç¡€ä½¿ç”¨

#### ç»Ÿä¸€PointLKæ¥å£
```python
from bridge.unified_pointlk import UnifiedPointLK

# åˆ›å»ºæ”¹è¿›ç‰ˆPointNetLKæ¨¡å‹ï¼ˆé»˜è®¤PointNetç‰¹å¾æå–å™¨ï¼‰
model = UnifiedPointLK(
    model_type='improved',
    feature_extractor='pointnet',
    dim_k=1024
)

# ğŸ†• ä½¿ç”¨CFormerç‰¹å¾æå–å™¨
model = UnifiedPointLK(
    model_type='improved',
    feature_extractor='cformer',
    feature_extractor_kwargs={
        'dim': 512,
        'num_heads': 8,
        'num_blocks': 6
    }
)

# åˆ›å»ºåŸç‰ˆPointNetLKæ¨¡å‹
model = UnifiedPointLK(
    model_type='original',
    feature_extractor='pointnet'
)

# ç‚¹äº‘é…å‡†
p0 = torch.randn(B, N, 3)  # æºç‚¹äº‘
p1 = torch.randn(B, N, 3)  # ç›®æ ‡ç‚¹äº‘
residual, transformation = model(p0, p1)
```

### ğŸ†• ç‰¹å¾æå–å™¨ä½¿ç”¨

#### ç›´æ¥ä½¿ç”¨ç‰¹å¾æå–å™¨
```python
from feature_extractors import create_feature_extractor

# åˆ›å»ºä¸åŒç±»å‹çš„ç‰¹å¾æå–å™¨
pointnet = create_feature_extractor('pointnet', dim_k=1024)
attention = create_feature_extractor('attention', 
                                   dim_k=1024, 
                                   num_heads=8, 
                                   num_blocks=4)
cformer = create_feature_extractor('cformer',
                                 dim=512,
                                 num_heads=8,
                                 num_blocks=6)
mamba3d = create_feature_extractor('mamba3d',
                                 dim=512,
                                 num_layers=6)

# ç‰¹å¾æå–
points = torch.randn(B, N, 3)
features = cformer(points)  # [B, dim_k]
```

#### ç‰¹å¾æå–å™¨å·¥å‚æ¨¡å¼
```python
from feature_extractors.factory import FeatureExtractorFactory

# è·å–å¯ç”¨çš„ç‰¹å¾æå–å™¨
available = FeatureExtractorFactory.list_available()
print(f"Available extractors: {available}")

# åŠ¨æ€åˆ›å»ºç‰¹å¾æå–å™¨
extractor_name = 'attention'
extractor = FeatureExtractorFactory.create(
    extractor_name,
    dim_k=1024,
    num_heads=8
)

# éªŒè¯å…¼å®¹æ€§
is_compatible = FeatureExtractorFactory.validate_compatibility(
    extractor_name, 
    model_type='improved'
)
```

### æ¨¡å‹æ¯”è¾ƒ

#### æ€§èƒ½å¯¹æ¯”åˆ†æ
```python
from comparison.model_comparison import ModelComparison

# åˆ›å»ºå¯¹æ¯”åˆ†æå™¨
comparator = ModelComparison()

# æ·»åŠ æ¨¡å‹
comparator.add_model('improved_pointnet', model_improved_pointnet)
comparator.add_model('improved_cformer', model_improved_cformer)
comparator.add_model('original_pointnet', model_original_pointnet)

# è¿è¡Œå¯¹æ¯”æµ‹è¯•
results = comparator.compare_models(test_data)

# ç”ŸæˆæŠ¥å‘Š
comparator.generate_report(results, output_path='./comparison_report.html')
```

#### ğŸ†• ç‰¹å¾æå–å™¨å¯¹æ¯”
```python
from comparison.feature_extractor_comparison import FeatureExtractorComparison

# ç‰¹å¾æå–å™¨æ€§èƒ½å¯¹æ¯”
fe_comparator = FeatureExtractorComparison()

extractors = ['pointnet', 'attention', 'cformer', 'mamba3d']
comparison_results = fe_comparator.compare_extractors(
    extractors, 
    test_data,
    metrics=['accuracy', 'speed', 'memory']
)

# å¯è§†åŒ–å¯¹æ¯”ç»“æœ
fe_comparator.plot_comparison(comparison_results)
```

### è‡ªå®šä¹‰æ‰©å±•

#### ğŸ†• åˆ›å»ºè‡ªå®šä¹‰ç‰¹å¾æå–å™¨
```python
from feature_extractors.base import BaseFeatureExtractor
import torch.nn as nn

class CustomFeatureExtractor(BaseFeatureExtractor):
    def __init__(self, dim_k=1024, custom_param=128):
        super().__init__(dim_k)
        self.custom_param = custom_param
        
        # è‡ªå®šä¹‰ç½‘ç»œæ¶æ„
        self.conv1 = nn.Conv1d(3, 64, 1)
        self.conv2 = nn.Conv1d(64, custom_param, 1)
        self.conv3 = nn.Conv1d(custom_param, dim_k, 1)
        
        # ç»§æ‰¿å¿…éœ€çš„å±æ€§
        self.t_out_t2 = dim_k // 2
        self.t_out_h1 = dim_k // 2
        
    def forward(self, points):
        """
        ç‰¹å¾æå–å‰å‘ä¼ æ’­
        
        Args:
            points: [B, N, 3] è¾“å…¥ç‚¹äº‘
            
        Returns:
            features: [B, dim_k] æå–çš„ç‰¹å¾
        """
        B, N, _ = points.shape
        
        # è½¬æ¢ä¸º[B, 3, N]æ ¼å¼
        x = points.transpose(2, 1)
        
        # ç‰¹å¾æå–
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.conv3(x)
        
        # å…¨å±€æ± åŒ–
        features = torch.max(x, 2)[0]  # [B, dim_k]
        
        return features

# æ³¨å†Œè‡ªå®šä¹‰ç‰¹å¾æå–å™¨
from feature_extractors.factory import FeatureExtractorFactory
FeatureExtractorFactory.register('custom', CustomFeatureExtractor)

# ä½¿ç”¨è‡ªå®šä¹‰ç‰¹å¾æå–å™¨
custom_extractor = create_feature_extractor(
    'custom',
    dim_k=1024,
    custom_param=256
)
```

### è®­ç»ƒå’Œæµ‹è¯•é›†æˆ

#### ğŸ†• ä½¿ç”¨è®­ç»ƒå™¨ç±»
```python
from trainer import Trainer

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model_type='improved',
    feature_extractor='cformer',
    dataset_type='c3vd',
    batch_size=16,
    learning_rate=1e-4
)

# è®­ç»ƒæ¨¡å‹
trainer.train(
    epochs=100,
    save_path='./models/cformer_model.pth',
    log_interval=10
)

# æµ‹è¯•æ¨¡å‹
test_results = trainer.test(
    model_path='./models/cformer_model.pth',
    test_data=test_loader
)
```

#### æ‰¹é‡è®­ç»ƒä¸åŒç‰¹å¾æå–å™¨
```python
from utils import batch_train_feature_extractors

# æ‰¹é‡è®­ç»ƒé…ç½®
extractors_config = {
    'pointnet': {'dim_k': 1024},
    'attention': {'dim_k': 1024, 'num_heads': 8},
    'cformer': {'dim': 512, 'num_heads': 8},
    'mamba3d': {'dim': 512, 'num_layers': 6}
}

# æ‰¹é‡è®­ç»ƒ
results = batch_train_feature_extractors(
    extractors_config,
    dataset='c3vd',
    epochs=50,
    output_dir='./batch_training_results'
)
```

### æ•°æ®å¤„ç†å·¥å…·

#### C3VDæ•°æ®å¤„ç†
```python
from data_utils import C3VDDataset, create_c3vd_pairs

# åˆ›å»ºC3VDæ•°æ®é›†
dataset = C3VDDataset(
    data_path='/path/to/C3VD_datasets',
    pairing_strategy='all',  # 'all', 'scene_reference', 'one_to_one'
    transform_magnitude=0.6,
    voxel_grid_size=64
)

# è‡ªå®šä¹‰æ•°æ®é…å¯¹
pairs = create_c3vd_pairs(
    data_path='/path/to/C3VD_datasets',
    strategy='custom',
    custom_pairs=[(scene1, frame1, scene2, frame2), ...]
)
```

#### ğŸ†• ä½“ç´ åŒ–æ§åˆ¶
```python
from data_utils import VoxelizationController

# åˆ›å»ºä½“ç´ åŒ–æ§åˆ¶å™¨
voxel_controller = VoxelizationController(
    voxel_size=0.03,
    timing='after_transform',  # 'before_transform' or 'after_transform'
    adaptive=True
)

# åº”ç”¨ä½“ç´ åŒ–
p0_voxelized = voxel_controller.voxelize(p0, timing='before')
p1_voxelized = voxel_controller.voxelize(p1, timing='after')
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### å¯¼å…¥é”™è¯¯
```bash
# é—®é¢˜ï¼šModuleNotFoundError: No module named 'feature_extractors'
# è§£å†³ï¼šç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
export PYTHONPATH=$PYTHONPATH:/path/to/PointNetLK_compare
```

#### å†…å­˜ä¸è¶³
```bash
# é—®é¢˜ï¼šCUDA out of memory
# è§£å†³ï¼šé™ä½æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
python train_unified.py --batch-size 4 --accumulate-grad-batches 4
```

#### ç‰¹å¾æå–å™¨å…¼å®¹æ€§
```python
# é—®é¢˜ï¼šç‰¹å¾æå–å™¨ä¸å…¼å®¹æŸä¸ªæ¨¡å‹ç±»å‹
# è§£å†³ï¼šæ£€æŸ¥æ”¯æŒçš„ç»„åˆ
from feature_extractors.factory import FeatureExtractorFactory
compatible = FeatureExtractorFactory.validate_compatibility('cformer', 'improved')
```

#### C3VDæ•°æ®é›†è·¯å¾„é—®é¢˜
```bash
# é—®é¢˜ï¼šæ•°æ®é›†è·¯å¾„é”™è¯¯
# è§£å†³ï¼šç¡®ä¿æ•°æ®é›†ç»“æ„æ­£ç¡®
C3VD_datasets/
â”œâ”€â”€ colon_1/
â”‚   â”œâ”€â”€ depth/
â”‚   â””â”€â”€ pose/
â”œâ”€â”€ colon_2/
â””â”€â”€ ...
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### è®­ç»ƒä¼˜åŒ–
- **æ‰¹æ¬¡å¤§å°**: AttentionNetæ¨è4-8ï¼ŒCFormeræ¨è8-16ï¼ŒPointNetå¯ç”¨16-32
- **å­¦ä¹ ç‡**: ç‰¹å¾æå–å™¨å¤æ‚åº¦è¶Šé«˜ï¼Œå»ºè®®ä½¿ç”¨è¶Šå°çš„å­¦ä¹ ç‡
- **å†…å­˜ç®¡ç†**: å¤§ç‰¹å¾æå–å™¨å»ºè®®å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

#### æ¨ç†ä¼˜åŒ–
- **æ¨¡å‹é‡åŒ–**: æ”¯æŒFP16æ¨ç†ï¼Œå¯æ˜¾è‘—å‡å°‘å†…å­˜ä½¿ç”¨
- **æ‰¹é‡æ¨ç†**: ä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡æé«˜GPUåˆ©ç”¨ç‡
- **ç‰¹å¾ç¼“å­˜**: åœ¨å¤šæ¬¡æµ‹è¯•ä¸­ç¼“å­˜ç‰¹å¾ä»¥æé«˜æ•ˆç‡

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# 1. Forkå¹¶å…‹éš†ä»“åº“
git clone https://github.com/yourusername/PointNetLK_compare.git
cd PointNetLK_compare

# 2. åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b feature/your-feature-name

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements.txt
pip install -e .  # å¯ç¼–è¾‘å®‰è£…
```

### ğŸ†• æ·»åŠ æ–°ç‰¹å¾æå–å™¨
```python
# 1. åœ¨feature_extractors/ç›®å½•åˆ›å»ºæ–°æ–‡ä»¶
# 2. ç»§æ‰¿BaseFeatureExtractor
# 3. å®ç°forwardæ–¹æ³•
# 4. åœ¨factory.pyä¸­æ³¨å†Œ
# 5. æ·»åŠ å•å…ƒæµ‹è¯•
# 6. æ›´æ–°æ–‡æ¡£
```

### ä»£ç è§„èŒƒ
- **ä»£ç é£æ ¼**: éµå¾ªPEP 8æ ‡å‡†
- **æ–‡æ¡£**: ä½¿ç”¨ä¸­æ–‡æ³¨é‡Šï¼Œè‹±æ–‡ä»£ç 
- **æµ‹è¯•**: æ–°åŠŸèƒ½å¿…é¡»åŒ…å«å•å…ƒæµ‹è¯•
- **ç±»å‹æç¤º**: å»ºè®®ä½¿ç”¨ç±»å‹æç¤º

### æäº¤æµç¨‹
```bash
# 1. è¿è¡Œæµ‹è¯•
python -m pytest tests/

# 2. æ£€æŸ¥ä»£ç é£æ ¼
flake8 feature_extractors/

# 3. æäº¤æ›´æ”¹
git add .
git commit -m "feat: add new feature extractor"

# 4. æ¨é€å¹¶åˆ›å»ºPR
git push origin feature/your-feature-name
```

### æŠ¥å‘ŠIssue
åœ¨æŠ¥å‘Šé—®é¢˜æ—¶ï¼Œè¯·åŒ…å«ï¼š
- **ç¯å¢ƒä¿¡æ¯**: Pythonç‰ˆæœ¬ã€PyTorchç‰ˆæœ¬ã€GPUä¿¡æ¯
- **å¤ç°æ­¥éª¤**: è¯¦ç»†çš„å¤ç°æ­¥éª¤
- **é”™è¯¯ä¿¡æ¯**: å®Œæ•´çš„é”™è¯¯å †æ ˆ
- **é¢„æœŸè¡Œä¸º**: æœŸæœ›çš„æ­£ç¡®è¡Œä¸º

---

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### C3VDæ•°æ®é›†æ€§èƒ½å¯¹æ¯”

| ç‰¹å¾æå–å™¨ | æ¨¡å‹ç±»å‹ | æ—‹è½¬è¯¯å·®(Â°) | å¹³ç§»è¯¯å·® | è®­ç»ƒæ—¶é—´(h) | æ¨ç†é€Ÿåº¦(ms) |
|------------|----------|-------------|----------|-------------|--------------|
| PointNet | Improved | 2.8 Â± 0.5 | 0.052 Â± 0.008 | 2.1 | 15 |
| PointNet | Original | 3.2 Â± 0.6 | 0.058 Â± 0.010 | 1.8 | 18 |
| AttentionNet | Improved | 2.1 Â± 0.4 | 0.041 Â± 0.006 | 6.5 | 45 |
| CFormer | Improved | 2.3 Â± 0.4 | 0.045 Â± 0.007 | 4.8 | 35 |
| FastAttention | Improved | 2.5 Â± 0.5 | 0.048 Â± 0.007 | 3.2 | 22 |
| Mamba3D | Improved | 2.4 Â± 0.4 | 0.046 Â± 0.007 | 4.1 | 28 |

### ModelNet40æ•°æ®é›†æ€§èƒ½åŸºå‡†

| ç‰¹å¾æå–å™¨ | RMSE(R) | RMSE(t) | MAE(R) | MAE(t) | æˆåŠŸç‡(%) |
|------------|---------|---------|---------|---------|-----------|
| PointNet | 3.78 | 0.043 | 1.32 | 0.021 | 88.5 |
| AttentionNet | 2.91 | 0.035 | 0.98 | 0.018 | 93.2 |
| CFormer | 3.15 | 0.038 | 1.12 | 0.019 | 91.7 |
| Mamba3D | 3.02 | 0.037 | 1.05 | 0.018 | 92.4 |

---

## ğŸ“š ç›¸å…³è®ºæ–‡

### æ ¸å¿ƒè®ºæ–‡
```bibtex
@inproceedings{li2021pointnetlk,
  title={PointNetLK Revisited},
  author={Li, Xueqian and Pontes, Jhony Kaesemodel and Lucey, Simon},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12763--12772},
  year={2021}
}

@inproceedings{aoki2019pointnetlk,
  title={PointNetLK: Robust \& efficient point cloud registration using PointNet},
  author={Aoki, Yasuhiro and Goforth, Hunter and Srivatsan, Rangaprasad Arun and Lucey, Simon},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7163--7172},
  year={2019}
}
```

### ğŸ†• ç‰¹å¾æå–å™¨ç›¸å…³è®ºæ–‡
```bibtex
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}
```

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®å’Œç ”ç©¶è€…çš„è´¡çŒ®ï¼š

- **åŸå§‹PointNetLK**: [Yasuhiro Aoki](https://github.com/hmgoforth/PointNetLK) ç­‰äººçš„å¼€åˆ›æ€§å·¥ä½œ
- **PointNetLK Revisited**: [Xueqian Li](https://github.com/Lilac-Lee/PointNetLK_Revisited) ç­‰äººçš„æ”¹è¿›å·¥ä½œ  
- **C3VDæ•°æ®é›†**: æä¾›äº†å®è´µçš„åŒ»å­¦å†…çª¥é•œæ•°æ®
- **PyTorchç¤¾åŒº**: æä¾›äº†ä¼˜ç§€çš„æ·±åº¦å­¦ä¹ æ¡†æ¶
- ğŸ†• **Transformerç¤¾åŒº**: ä¸ºæ³¨æ„åŠ›æœºåˆ¶å’ŒTransformeræ¶æ„çš„å‘å±•åšå‡ºè´¡çŒ®
- ğŸ†• **State Space Models**: Mambaå’Œç›¸å…³çŠ¶æ€ç©ºé—´æ¨¡å‹çš„ç ”ç©¶è€…

ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®æä¾›åé¦ˆã€å»ºè®®å’Œè´¡çŒ®çš„ç ”ç©¶è€…å’Œå¼€å‘è€…ï¼

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ **MITè®¸å¯è¯** è¿›è¡Œè®¸å¯ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

```
MIT License

Copyright (c) 2024 PointNetLK Compare Project

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ“§ è”ç³»æ–¹å¼

- **é¡¹ç›®ç»´æŠ¤è€…**: [è”ç³»ä¿¡æ¯]
- **Issueåé¦ˆ**: [GitHub Issues](https://github.com/yourusername/PointNetLK_compare/issues)
- **åŠŸèƒ½è¯·æ±‚**: [GitHub Discussions](https://github.com/yourusername/PointNetLK_compare/discussions)
- **é‚®ä»¶è”ç³»**: your.email@example.com

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªStarï¼ â­**

**ğŸ”„ æ¬¢è¿Forkå’Œè´¡çŒ®ä»£ç ï¼ ğŸ”„**

**ğŸ“¢ æ¬¢è¿åˆ†äº«ç»™æ›´å¤šéœ€è¦çš„ç ”ç©¶è€…ï¼ ğŸ“¢**

</div>
