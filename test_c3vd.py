#!/usr/bin/env python3
"""
C3VDæ•°æ®é›†ä¸“ç”¨æµ‹è¯•è„šæœ¬
Dedicated testing script for C3VD dataset with voxelization support
"""

import argparse
import os
import sys
import numpy as np
import torch
import torch.utils.data
import time
from pathlib import Path
import logging

# å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
from data_utils import create_c3vd_dataset
from bridge import ModelBridge

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
LOGGER = logging.getLogger(__name__)


def parse_arguments():
    """è§£æC3VDæµ‹è¯•å‚æ•°"""
    parser = argparse.ArgumentParser(description='C3VDæ•°æ®é›†PointNetLKæµ‹è¯•è„šæœ¬')
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('--c3vd-root', required=True, type=str,
                        help='C3VDæ•°æ®é›†æ ¹ç›®å½•è·¯å¾„')
    parser.add_argument('--model-path', required=True, type=str,
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--output-dir', required=True, type=str,
                        help='æµ‹è¯•ç»“æœè¾“å‡ºç›®å½•')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--model-type', default='improved', choices=['original', 'improved'],
                        help='æ¨¡å‹ç±»å‹: original(åŸç‰ˆ) æˆ– improved(æ”¹è¿›ç‰ˆ)')
    parser.add_argument('--dim-k', default=1024, type=int,
                        help='ç‰¹å¾å‘é‡ç»´åº¦')
    parser.add_argument('--max-iter', default=10, type=int,
                        help='LKç®—æ³•æœ€å¤§è¿­ä»£æ¬¡æ•°')
    
    # åŸç‰ˆæ¨¡å‹ç‰¹æœ‰å‚æ•°
    parser.add_argument('--delta', default=1.0e-2, type=float,
                        help='æ•°å€¼é›…å¯æ¯”æ­¥é•¿ï¼ˆä»…åŸç‰ˆï¼‰')
    
    # C3VDæ•°æ®é›†é…ç½®
    parser.add_argument('--source-subdir', default='C3VD_ply_source', type=str,
                        help='æºç‚¹äº‘å­ç›®å½•åç§°')
    parser.add_argument('--target-subdir', default='visible_point_cloud_ply_depth', type=str,
                        help='ç›®æ ‡ç‚¹äº‘å­ç›®å½•åç§°')
    parser.add_argument('--pairing-strategy', default='one_to_one',
                        choices=['one_to_one', 'scene_reference', 'source_to_source', 'target_to_target', 'all'],
                        help='ç‚¹äº‘é…å¯¹ç­–ç•¥')
    
    # æµ‹è¯•é…ç½®
    parser.add_argument('--test-transform-mags', default='0.2,0.4,0.6,0.8', type=str,
                        help='æµ‹è¯•å˜æ¢å¹…åº¦åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--batch-size', default=8, type=int,
                        help='æµ‹è¯•æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num-points', default=1024, type=int,
                        help='é‡‡æ ·ç‚¹æ•°')
    parser.add_argument('--device', default='cuda:0', type=str,
                        help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--workers', default=4, type=int,
                        help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°')
    
    # ä½“ç´ åŒ–é…ç½®
    parser.add_argument('--voxel-size', default=0.05, type=float,
                        help='ä½“ç´ å¤§å°')
    parser.add_argument('--voxel-grid-size', default=32, type=int,
                        help='ä½“ç´ ç½‘æ ¼å¤§å°')
    parser.add_argument('--max-voxel-points', default=100, type=int,
                        help='æ¯ä¸ªä½“ç´ æœ€å¤§ç‚¹æ•°')
    parser.add_argument('--max-voxels', default=20000, type=int,
                        help='æœ€å¤§ä½“ç´ æ•°é‡')
    parser.add_argument('--min-voxel-points-ratio', default=0.1, type=float,
                        help='æœ€å°ä½“ç´ ç‚¹æ•°æ¯”ä¾‹')
    
    # è¯„ä¼°é…ç½®
    parser.add_argument('--save-results', action='store_true',
                        help='æ˜¯å¦ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœ')
    parser.add_argument('--visualize', action='store_true',
                        help='æ˜¯å¦ç”Ÿæˆå¯è§†åŒ–ç»“æœ')
    
    return parser.parse_args()


class C3VDTester:
    """C3VDæµ‹è¯•å™¨"""
    
    def __init__(self, args):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.args = args
        self.device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(args.output_dir, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        log_file = os.path.join(args.output_dir, 'test_results.log')
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        LOGGER.addHandler(file_handler)
        
        LOGGER.info(f"å¼€å§‹C3VDæ•°æ®é›†æµ‹è¯•")
        LOGGER.info(f"å‚æ•°: {vars(args)}")
        
        # éªŒè¯æ•°æ®é›†
        self.source_path, self.target_path = self._validate_dataset()
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()
        
        # è§£ææµ‹è¯•å˜æ¢å¹…åº¦
        self.test_mags = [float(x.strip()) for x in args.test_transform_mags.split(',')]
        LOGGER.info(f"æµ‹è¯•å˜æ¢å¹…åº¦: {self.test_mags}")
    
    def _validate_dataset(self):
        """éªŒè¯æ•°æ®é›†ç»“æ„"""
        c3vd_path = Path(self.args.c3vd_root)
        
        if not c3vd_path.exists():
            raise FileNotFoundError(f"C3VDæ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {self.args.c3vd_root}")
        
        source_path = c3vd_path / self.args.source_subdir
        target_path = c3vd_path / self.args.target_subdir
        
        if not source_path.exists():
            raise FileNotFoundError(f"æºç‚¹äº‘ç›®å½•ä¸å­˜åœ¨: {source_path}")
        
        if not target_path.exists():
            raise FileNotFoundError(f"ç›®æ ‡ç‚¹äº‘ç›®å½•ä¸å­˜åœ¨: {target_path}")
        
        LOGGER.info(f"æ•°æ®é›†éªŒè¯é€šè¿‡: {source_path}, {target_path}")
        return str(source_path), str(target_path)
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        LOGGER.info(f"åŠ è½½æ¨¡å‹: {self.args.model_path}")
        
        # åˆ›å»ºæ¨¡å‹
        model_kwargs = {
            'dim_k': self.args.dim_k,
        }
        
        if self.args.model_type == 'original':
            model_kwargs.update({
                'delta': self.args.delta,
                'learn_delta': False,  # æµ‹è¯•æ—¶ä¸å­¦ä¹ 
            })
        
        model = ModelBridge(self.args.model_type, **model_kwargs)
        
        # åŠ è½½æƒé‡
        if not os.path.exists(self.args.model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.args.model_path}")
        
        checkpoint = torch.load(self.args.model_path, map_location=self.device)
        
        # å¤„ç†ä¸åŒçš„ä¿å­˜æ ¼å¼
        if 'model' in checkpoint:
            model.load_state_dict(checkpoint['model'])
        else:
            model.load_state_dict(checkpoint)
        
        model.to(self.device)
        model.eval()
        
        LOGGER.info(f"æ¨¡å‹åŠ è½½å®Œæˆå¹¶ç§»åŠ¨åˆ° {self.device}")
        return model
    
    def _create_test_dataset(self, transform_mag):
        """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
        # ä½“ç´ åŒ–é…ç½®
        voxel_config = {
            'voxel_size': self.args.voxel_size,
            'voxel_grid_size': self.args.voxel_grid_size,
            'max_voxel_points': self.args.max_voxel_points,
            'max_voxels': self.args.max_voxels,
            'min_voxel_points_ratio': self.args.min_voxel_points_ratio
        }
        
        # æ™ºèƒ½é‡‡æ ·é…ç½®
        sampling_config = {
            'target_points': self.args.num_points,
            'intersection_priority': True,
            'min_intersection_ratio': 0.3,
            'max_intersection_ratio': 0.7
        }
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        testset = create_c3vd_dataset(
            source_root=self.source_path,
            target_root=self.target_path,
            pairing_strategy=self.args.pairing_strategy,
            mag=transform_mag,
            train=False,  # æµ‹è¯•æ¨¡å¼
            vis=self.args.visualize,
            voxel_config=voxel_config,
            sampling_config=sampling_config
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader = torch.utils.data.DataLoader(
            testset,
            batch_size=self.args.batch_size,
            shuffle=False,
            num_workers=self.args.workers,
            pin_memory=True,
            drop_last=False
        )
        
        return testset, test_loader
    
    def _compute_metrics(self, pred_transform, gt_transform):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if torch.is_tensor(pred_transform):
            pred_transform = pred_transform.detach().cpu().numpy()
        if torch.is_tensor(gt_transform):
            gt_transform = gt_transform.detach().cpu().numpy()
        
        batch_size = pred_transform.shape[0]
        
        # è®¡ç®—æ—‹è½¬å’Œå¹³ç§»è¯¯å·®
        rotation_errors = []
        translation_errors = []
        
        for i in range(batch_size):
            pred_R = pred_transform[i, :3, :3]
            pred_t = pred_transform[i, :3, 3]
            gt_R = gt_transform[i, :3, :3]
            gt_t = gt_transform[i, :3, 3]
            
            # æ—‹è½¬è¯¯å·®ï¼ˆè§’åº¦ï¼‰
            R_diff = np.dot(pred_R, gt_R.T)
            trace_R = np.trace(R_diff)
            # ç¡®ä¿traceåœ¨æœ‰æ•ˆèŒƒå›´å†…
            trace_R = np.clip(trace_R, -1.0, 3.0)
            rotation_error = np.arccos((trace_R - 1) / 2) * 180 / np.pi
            rotation_errors.append(rotation_error)
            
            # å¹³ç§»è¯¯å·®ï¼ˆæ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰
            translation_error = np.linalg.norm(pred_t - gt_t)
            translation_errors.append(translation_error)
        
        return np.array(rotation_errors), np.array(translation_errors)
    
    def test_single_magnitude(self, transform_mag):
        """æµ‹è¯•å•ä¸ªå˜æ¢å¹…åº¦"""
        LOGGER.info(f"æµ‹è¯•å˜æ¢å¹…åº¦: {transform_mag}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        testset, test_loader = self._create_test_dataset(transform_mag)
        
        # æµ‹è¯•ç»Ÿè®¡
        total_samples = 0
        total_rotation_error = 0.0
        total_translation_error = 0.0
        all_rotation_errors = []
        all_translation_errors = []
        all_intersection_ratios = []
        
        start_time = time.time()
        
        with torch.no_grad():
            for batch_idx, data in enumerate(test_loader):
                # è§£ææ•°æ®
                source = data['source'].to(self.device)
                target = data['target'].to(self.device)
                igt = data['igt'].to(self.device)
                
                # è·å–äº¤é›†æ¯”ä¾‹ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'intersection_ratio' in data:
                    intersection_ratios = data['intersection_ratio'].cpu().numpy()
                    all_intersection_ratios.extend(intersection_ratios)
                
                try:
                    # æ¨¡å‹æ¨ç†
                    if self.args.model_type == 'improved':
                        # æ”¹è¿›ç‰ˆéœ€è¦å¯ç”¨æ¢¯åº¦è®¡ç®—
                        source.requires_grad_(True)
                        target.requires_grad_(True)
                        with torch.enable_grad():
                            pred_transform = self.model.forward(source, target, maxiter=self.args.max_iter)
                    else:
                        pred_transform = self.model.forward(source, target, maxiter=self.args.max_iter)
                    
                    # è®¡ç®—è¯¯å·®
                    rotation_errors, translation_errors = self._compute_metrics(pred_transform, igt)
                    
                    # ç»Ÿè®¡
                    total_samples += len(rotation_errors)
                    total_rotation_error += np.sum(rotation_errors)
                    total_translation_error += np.sum(translation_errors)
                    all_rotation_errors.extend(rotation_errors)
                    all_translation_errors.extend(translation_errors)
                    
                    # æ‰“å°è¿›åº¦
                    if batch_idx % 10 == 0:
                        LOGGER.info(f"  æ‰¹æ¬¡ {batch_idx}/{len(test_loader)}, "
                                   f"å¹³å‡æ—‹è½¬è¯¯å·®: {np.mean(rotation_errors):.3f}Â°, "
                                   f"å¹³å‡å¹³ç§»è¯¯å·®: {np.mean(translation_errors):.4f}")
                
                except Exception as e:
                    LOGGER.warning(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        test_time = time.time() - start_time
        
        if total_samples > 0:
            avg_rotation_error = total_rotation_error / total_samples
            avg_translation_error = total_translation_error / total_samples
            
            # è®¡ç®—ä¸­ä½æ•°å’Œæ ‡å‡†å·®
            median_rotation_error = np.median(all_rotation_errors)
            std_rotation_error = np.std(all_rotation_errors)
            median_translation_error = np.median(all_translation_errors)
            std_translation_error = np.std(all_translation_errors)
            
            # è®¡ç®—æˆåŠŸç‡ï¼ˆæ—‹è½¬è¯¯å·®<5åº¦ï¼Œå¹³ç§»è¯¯å·®<0.1ï¼‰
            success_rotation = np.sum(np.array(all_rotation_errors) < 5.0) / len(all_rotation_errors)
            success_translation = np.sum(np.array(all_translation_errors) < 0.1) / len(all_translation_errors)
            success_overall = np.sum((np.array(all_rotation_errors) < 5.0) & 
                                   (np.array(all_translation_errors) < 0.1)) / len(all_rotation_errors)
            
            results = {
                'transform_mag': transform_mag,
                'total_samples': total_samples,
                'avg_rotation_error': avg_rotation_error,
                'avg_translation_error': avg_translation_error,
                'median_rotation_error': median_rotation_error,
                'median_translation_error': median_translation_error,
                'std_rotation_error': std_rotation_error,
                'std_translation_error': std_translation_error,
                'success_rotation': success_rotation,
                'success_translation': success_translation,
                'success_overall': success_overall,
                'test_time': test_time,
                'all_rotation_errors': all_rotation_errors,
                'all_translation_errors': all_translation_errors,
                'all_intersection_ratios': all_intersection_ratios
            }
            
            LOGGER.info(f"å˜æ¢å¹…åº¦ {transform_mag} æµ‹è¯•å®Œæˆ:")
            LOGGER.info(f"  æ ·æœ¬æ•°: {total_samples}")
            LOGGER.info(f"  å¹³å‡æ—‹è½¬è¯¯å·®: {avg_rotation_error:.3f}Â° (ä¸­ä½æ•°: {median_rotation_error:.3f}Â°)")
            LOGGER.info(f"  å¹³å‡å¹³ç§»è¯¯å·®: {avg_translation_error:.4f} (ä¸­ä½æ•°: {median_translation_error:.4f})")
            LOGGER.info(f"  æˆåŠŸç‡: {success_overall:.1%} (æ—‹è½¬<5Â°ä¸”å¹³ç§»<0.1)")
            LOGGER.info(f"  æµ‹è¯•æ—¶é—´: {test_time:.2f}s")
            
            return results
        else:
            LOGGER.error(f"å˜æ¢å¹…åº¦ {transform_mag} æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
            return None
    
    def run_comprehensive_test(self):
        """è¿è¡Œå…¨é¢æµ‹è¯•"""
        LOGGER.info("å¼€å§‹å…¨é¢æµ‹è¯•...")
        
        all_results = []
        
        for mag in self.test_mags:
            result = self.test_single_magnitude(mag)
            if result:
                all_results.append(result)
        
        # ä¿å­˜ç»“æœ
        if self.args.save_results:
            self._save_results(all_results)
        
        # æ‰“å°æ€»ç»“
        self._print_summary(all_results)
        
        return all_results
    
    def _save_results(self, results):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        import json
        
        # ä¿å­˜JSONæ ¼å¼ç»“æœ
        json_results = []
        for result in results:
            json_result = result.copy()
            # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨
            json_result['all_rotation_errors'] = [float(x) for x in result['all_rotation_errors']]
            json_result['all_translation_errors'] = [float(x) for x in result['all_translation_errors']]
            json_result['all_intersection_ratios'] = [float(x) for x in result['all_intersection_ratios']]
            json_results.append(json_result)
        
        json_file = os.path.join(self.args.output_dir, 'test_results.json')
        with open(json_file, 'w') as f:
            json.dump(json_results, f, indent=2)
        
        LOGGER.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
        # ä¿å­˜CSVæ ¼å¼æ‘˜è¦
        csv_file = os.path.join(self.args.output_dir, 'test_summary.csv')
        with open(csv_file, 'w') as f:
            f.write("transform_mag,samples,avg_rot_error,avg_trans_error,median_rot_error,median_trans_error,success_rate\n")
            for result in results:
                f.write(f"{result['transform_mag']},{result['total_samples']},"
                       f"{result['avg_rotation_error']:.3f},{result['avg_translation_error']:.4f},"
                       f"{result['median_rotation_error']:.3f},{result['median_translation_error']:.4f},"
                       f"{result['success_overall']:.3f}\n")
        
        LOGGER.info(f"æµ‹è¯•æ‘˜è¦å·²ä¿å­˜åˆ°: {csv_file}")
    
    def _print_summary(self, results):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*80)
        print("ğŸ¯ C3VDæ•°æ®é›†æµ‹è¯•æ€»ç»“")
        print("="*80)
        
        print(f"æ¨¡å‹ç±»å‹: {self.args.model_type}")
        print(f"æ¨¡å‹è·¯å¾„: {self.args.model_path}")
        print(f"æ•°æ®é›†è·¯å¾„: {self.args.c3vd_root}")
        print(f"é…å¯¹ç­–ç•¥: {self.args.pairing_strategy}")
        
        print("\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print("-"*80)
        print(f"{'å˜æ¢å¹…åº¦':<10} {'æ ·æœ¬æ•°':<8} {'æ—‹è½¬è¯¯å·®(Â°)':<12} {'å¹³ç§»è¯¯å·®':<10} {'æˆåŠŸç‡':<8}")
        print("-"*80)
        
        for result in results:
            print(f"{result['transform_mag']:<10.1f} {result['total_samples']:<8} "
                  f"{result['avg_rotation_error']:<12.3f} {result['avg_translation_error']:<10.4f} "
                  f"{result['success_overall']:<8.1%}")
        
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = C3VDTester(args)
        
        # è¿è¡Œæµ‹è¯•
        results = tester.run_comprehensive_test()
        
        if results:
            print("\nâœ… æµ‹è¯•å®Œæˆ!")
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥!")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main() 